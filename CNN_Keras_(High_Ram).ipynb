{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Keras (High Ram).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonJachjajev/CNN/blob/AntonCNN/CNN_Keras_(High_Ram).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IunVXpEVf0nk"
      },
      "source": [
        "#Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb77yZ9fzMG"
      },
      "source": [
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWsZ_Ao8f3DD"
      },
      "source": [
        "#Load csv data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "#upload truth data\r\n",
        "training_truth_data = pd.read_csv('/content/drive/MyDrive/isic-challenge-2017/ISIC-2017_Training_Part3_GroundTruth.csv') \r\n",
        "testing_truth_data = pd.read_csv('/content/drive/MyDrive/isic-challenge-2017/ISIC-2017_Test_v2_Part3_GroundTruth.csv') \r\n",
        "validation_truth_data = pd.read_csv('/content/drive/MyDrive/isic-challenge-2017/ISIC-2017_Validation_Part3_GroundTruth.csv')\r\n",
        "\r\n",
        "#upload metadata\r\n",
        "training_metadata = pd.read_csv('/content/drive/MyDrive/isic-challenge-2017/ISIC-2017_Training_Data/ISIC-2017_Training_Data/ISIC-2017_Training_Data_metadata.csv')\r\n",
        "testing_metadata = pd.read_csv('/content/drive/MyDrive/isic-challenge-2017/ISIC-2017_Test_v2_Data/ISIC-2017_Test_v2_Data/ISIC-2017_Test_v2_Data_metadata.csv')\r\n",
        "validation_metadata = pd.read_csv('/content/drive/MyDrive/isic-challenge-2017/ISIC-2017_Validation_Data/ISIC-2017_Validation_Data/ISIC-2017_Validation_Data_metadata.csv')\r\n",
        "\r\n",
        "# path to images\r\n",
        "path_training_images ='/content/drive/MyDrive/isic-challenge-2017/ISIC-2017_Training_Data/ISIC-2017_Training_Data_resized/'\r\n",
        "path_testing_images ='/content/drive/MyDrive/isic-challenge-2017/ISIC-2017_Test_v2_Data/ISIC-2017_Test_v2_Data_resized/'\r\n",
        "path_validation_images = '/content/drive/MyDrive/isic-challenge-2017/ISIC-2017_Validation_Data/ISIC-2017_Validation_Data_resized/'\r\n",
        "\r\n",
        "\r\n",
        "path_to_malignant_train = '/content/drive/MyDrive/Data/train/malignant/'\r\n",
        "path_to_malignant_test = '/content/drive/MyDrive/Data/test/malignant/'\r\n",
        "path_to_malignant_val =  '/content/drive/MyDrive/Data/validation/malignant/'\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwrJiGoff6Kz"
      },
      "source": [
        "#Hyper Parameters and constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUP6GHSff7DN"
      },
      "source": [
        "#constants\r\n",
        "size = 224\r\n",
        "image_resize = (size, size)\r\n",
        "input_shape = (size, size, 3)\r\n",
        "\r\n",
        "#model parameters\r\n",
        "batch_size = 32\r\n",
        "epochs = 100\r\n",
        "num_classes = 2\r\n",
        "\r\n",
        "#optimiser parameters\r\n",
        "learning_rate = 1e-4\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tlP52SXf9ez"
      },
      "source": [
        "#Import Data\r\n",
        "\r\n",
        "```\r\n",
        "1.   Training Data\r\n",
        "2.   Validation Data\r\n",
        "3.   Training Data\r\n",
        "\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2_rXse6f9_s"
      },
      "source": [
        "import pandas as pd \r\n",
        "#add new column to truth_data witht he path to corresponding image\r\n",
        "def prepare_data(truth_data, metadata, image_path):\r\n",
        "  path_to_images_array = []\r\n",
        "  for x in truth_data.values:\r\n",
        "    path_to_image = image_path+str(x[0])+'.jpg'\r\n",
        "    path_to_images_array.append(str(path_to_image))\r\n",
        "  #add path to image to the truth data frame\r\n",
        "  truth_data['path_to_image'] = path_to_images_array\r\n",
        "  #return merge truth data with metadata, join on the image_id's\r\n",
        "  return pd.merge(truth_data, metadata, left_on='image_id', right_on='image_id', how='left')\r\n",
        "\r\n",
        "#load data to dataframes\r\n",
        "training_data = prepare_data(training_truth_data, training_metadata, path_training_images)\r\n",
        "testing_data = prepare_data(testing_truth_data, testing_metadata, path_testing_images)\r\n",
        "validation_data = prepare_data(validation_truth_data, validation_metadata, path_validation_images)\r\n",
        "\r\n",
        "##show all data when printing\r\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\r\n",
        "pd.set_option('display.max_colwidth', None)\r\n",
        "print(training_data.head())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyQ-Fq-GgAZk"
      },
      "source": [
        "#Load Extra Data\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8W3fYkagB5D"
      },
      "source": [
        "from PIL import Image\r\n",
        "import os, sys\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "def getData(path, MAX):\r\n",
        "  IMG = []\r\n",
        "  dirs = os.listdir( path )\r\n",
        "  print(len(dirs))\r\n",
        "  counter = 0;\r\n",
        "  for item in dirs:\r\n",
        "    if (counter == MAX):\r\n",
        "      break\r\n",
        "    if os.path.isfile(path+item) and item.endswith('jpg'):\r\n",
        "        read = lambda imname: np.asarray(Image.open(path+item).convert(\"RGB\"))\r\n",
        "        img = read(path+item)\r\n",
        "        img = cv2.resize(img, image_resize)\r\n",
        "        IMG.append(np.array(img)/255.)\r\n",
        "        counter = counter+1\r\n",
        "  return IMG\r\n",
        "\r\n",
        "tain_data_x = np.array(getData(path_to_malignant_train,400))\r\n",
        "test_data_x = np.array(getData(path_to_malignant_test,200))\r\n",
        "val_data_x = np.array(getData(path_to_malignant_val,50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woE3UIzigDMj"
      },
      "source": [
        "#Image Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-I32OfgDj8"
      },
      "source": [
        "#loading pictures \r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "import time\r\n",
        "\r\n",
        "def Dataset_loader(DATAFRAME,VALUE,MAX=0):\r\n",
        "    '''\r\n",
        "    Dataframe - dataframe to extract data from\r\n",
        "    Value - 1 or 0 do we can idenitfy which image we loading benign or malignant\r\n",
        "    MAX - How many copies of malignant \r\n",
        "    '''\r\n",
        "    IMG = []\r\n",
        "    counter = 0;\r\n",
        "    for index, row in DATAFRAME.iterrows():\r\n",
        "      if (MAX!=0 and counter == MAX):\r\n",
        "        break\r\n",
        "      if (row[1] == VALUE):\r\n",
        "        read = lambda imname: np.asarray(Image.open(row[3]).convert(\"RGB\"))\r\n",
        "        img = read(row[3])\r\n",
        "        img = cv2.resize(img, image_resize)\r\n",
        "        IMG.append(np.array(img)/255.)\r\n",
        "        counter = counter + 1\r\n",
        "    return IMG\r\n",
        "\r\n",
        "print('Uploading Train Data')\r\n",
        "time1 = time.perf_counter()\r\n",
        "training_data = training_data.sample(frac=1)\r\n",
        "benign_train = np.array(Dataset_loader(training_data, 0.0,800))\r\n",
        "malign_train = np.array(Dataset_loader(training_data, 1.0))\r\n",
        "time2 = time.perf_counter()\r\n",
        "print(f'{time2-time1}s')\r\n",
        "print('Uploading Test Data')\r\n",
        "time1 = time.perf_counter()\r\n",
        "testing_data = testing_data.sample(frac=1)\r\n",
        "benign_test = np.array(Dataset_loader(testing_data,0.0,250))\r\n",
        "malign_test = np.array(Dataset_loader(testing_data,1.0))\r\n",
        "time2 = time.perf_counter()\r\n",
        "print(f'{time2-time1}s')\r\n",
        "print('Uploading Validation Data')\r\n",
        "time1 = time.perf_counter()\r\n",
        "validation_data = validation_data.sample(frac=1)\r\n",
        "benign_val = np.array(Dataset_loader(validation_data, 0.0,80))\r\n",
        "malign_val = np.array(Dataset_loader(validation_data, 1.0))\r\n",
        "time2 = time.perf_counter()\r\n",
        "print(f'{time2-time1}s')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRcwtNwAgGv9"
      },
      "source": [
        "#Generate Data with Labels\r\n",
        " Generates Labels, Merges image data with label data, and shuffles\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "```\r\n",
        "train 1416 [706 710]\r\n",
        "test 567 [250 317]\r\n",
        "val 158 [94 64]\r\n",
        "```\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRnz7IzqgH19"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "import collections\r\n",
        "# Create labels\r\n",
        "malign_train = np.concatenate((malign_train, tain_data_x), axis = 0)\r\n",
        "malign_test = np.concatenate((malign_test, test_data_x), axis = 0)\r\n",
        "malign_val = np.concatenate((malign_val, val_data_x), axis = 0)\r\n",
        "\r\n",
        "benign_train_label = np.zeros(len(benign_train))\r\n",
        "malign_train_label = np.ones(len(malign_train))\r\n",
        "benign_test_label = np.zeros(len(benign_test))\r\n",
        "malign_test_label = np.ones(len(malign_test))\r\n",
        "#benign_val_label = np.zeros(len(benign_val))\r\n",
        "#malign_val_label = np.ones(len(malign_val))\r\n",
        "\r\n",
        "# Merge data \r\n",
        "X_train = np.concatenate((benign_train, malign_train), axis = 0)\r\n",
        "Y_train = np.concatenate((benign_train_label, malign_train_label), axis = 0)\r\n",
        "X_test = np.concatenate((benign_test, malign_test), axis = 0)\r\n",
        "Y_test = np.concatenate((benign_test_label, malign_test_label), axis = 0)\r\n",
        "#X_val = np.concatenate((benign_val, malign_val), axis = 0)\r\n",
        "#Y_val = np.concatenate((benign_val_label, malign_val_label), axis = 0)\r\n",
        "\r\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, shuffle = True, random_state = 42)\r\n",
        "\r\n",
        "# Shuffle train data\r\n",
        "s = np.arange(X_train.shape[0])\r\n",
        "np.random.shuffle(s)\r\n",
        "X_train = X_train[s]\r\n",
        "Y_train = Y_train[s]\r\n",
        "\r\n",
        "# Shuffle test data\r\n",
        "s = np.arange(X_test.shape[0])\r\n",
        "np.random.shuffle(s)\r\n",
        "X_test = X_test[s]\r\n",
        "Y_test = Y_test[s]\r\n",
        "\r\n",
        "# Shuffle validation data\r\n",
        "s = np.arange(X_val.shape[0])\r\n",
        "np.random.shuffle(s)\r\n",
        "X_val = X_val[s]\r\n",
        "Y_val = Y_val[s]\r\n",
        "\r\n",
        "unique, counts = np.unique(Y_train, return_counts=True)\r\n",
        "print(f'train {len(X_train)} {counts}')\r\n",
        "unique, counts = np.unique(Y_test, return_counts=True)\r\n",
        "print(f'test {len(X_test)} {counts}')\r\n",
        "unique, counts = np.unique(Y_val, return_counts=True)\r\n",
        "print(f'val {len(X_val)} {counts}')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAzj-bW8gKU7"
      },
      "source": [
        "#Image data augmentation. Remove pieces of images randomly\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FtnK1uwgLaE"
      },
      "source": [
        "# method that takes random images and removes random sized rectangle from it\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\r\n",
        "    def eraser(input_img):\r\n",
        "        img_h, img_w, img_c = input_img.shape\r\n",
        "        p_1 = np.random.rand()\r\n",
        "\r\n",
        "        if p_1 > p:\r\n",
        "            return input_img\r\n",
        "\r\n",
        "        while True:\r\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\r\n",
        "            r = np.random.uniform(r_1, r_2)\r\n",
        "            w = int(np.sqrt(s / r))\r\n",
        "            h = int(np.sqrt(s * r))\r\n",
        "            left = np.random.randint(0, img_w)\r\n",
        "            top = np.random.randint(0, img_h)\r\n",
        "\r\n",
        "            if left + w <= img_w and top + h <= img_h:\r\n",
        "                break\r\n",
        "\r\n",
        "        if pixel_level:\r\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\r\n",
        "        else:\r\n",
        "            c = np.random.uniform(v_l, v_h)\r\n",
        "\r\n",
        "        input_img[top:top + h, left:left + w, :] = c\r\n",
        "\r\n",
        "        return input_img\r\n",
        "\r\n",
        "    return eraser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AjsTpe3gNty"
      },
      "source": [
        "#Image data augmentation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1UbNr_IgO_j"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "datagen = ImageDataGenerator(\r\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\r\n",
        "        samplewise_center=False,  # set each sample mean to 0\r\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\r\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\r\n",
        "        zca_whitening=False,  # apply ZCA whitening\r\n",
        "        rotation_range=180,  # randomly rotate images in the range (degrees, 0 to 180)\r\n",
        "        zoom_range = 0.3, # Randomly zoom image \r\n",
        "        width_shift_range=0.3,  # randomly shift images horizontally (fraction of total width)\r\n",
        "        height_shift_range=0.3,  # randomly shift images vertically (fraction of total height)\r\n",
        "        horizontal_flip=True,  # randomly flip images\r\n",
        "        vertical_flip=True,  # randomly flip images\r\n",
        "        preprocessing_function=get_random_eraser(p=0.5, v_h=0, s_h=0.25))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO-8jC3igSPp"
      },
      "source": [
        "#CNN architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz8nk_crgQyT"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\r\n",
        "from keras import layers\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras import regularizers \r\n",
        "import keras\r\n",
        "#Define CNN architecture\r\n",
        "model = keras.Sequential(\r\n",
        "    [\r\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\r\n",
        "        layers.MaxPooling2D(),\r\n",
        "        layers.Conv2D(16, (3, 3), activation='relu', padding='same'),       \r\n",
        "        layers.Conv2D(24, (3, 3), activation='relu', padding='same'),\r\n",
        "        layers.Conv2D(24, (3, 3), activation='relu', padding='same'),\r\n",
        "        layers.MaxPooling2D(),\r\n",
        "        layers.Conv2D(40, (5, 5), activation='relu', padding='same'),       \r\n",
        "        layers.Conv2D(40, (5, 5), activation='relu', padding='same'),\r\n",
        "        layers.MaxPooling2D(),\r\n",
        "        layers.Conv2D(80, (3, 3), activation='relu', padding='same'),\r\n",
        "        layers.Conv2D(80, (3, 3), activation='relu', padding='same'),\r\n",
        "        layers.MaxPooling2D(),\r\n",
        "        layers.Conv2D(112, (5, 5), activation='relu', padding='same'),\r\n",
        "        layers.Conv2D(112, (5, 5), activation='relu', padding='same'),       \r\n",
        "        layers.Conv2D(192, (5, 5), activation='relu', padding='same'),\r\n",
        "        layers.Conv2D(192, (5, 5), activation='relu', padding='same'), \r\n",
        "        layers.MaxPooling2D(),     \r\n",
        "        layers.Conv2D(640, (3, 3), activation='relu', padding='same'),\r\n",
        "        layers.Conv2D(640, (3, 3), activation='relu', padding='same'),\r\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),  \r\n",
        "        layers.Dropout(0.2),\r\n",
        "        layers.Dense(128, activation='relu'),\r\n",
        "        layers.Dropout(0.2),\r\n",
        "        layers.BatchNormalization(),\r\n",
        "        layers.Flatten(),\r\n",
        "        layers.Dense(1, activation='softmax')\r\n",
        "    ]\r\n",
        ")\r\n",
        "layers\r\n",
        "\r\n",
        "alpha = 1e-4  # weight decay coefficient\r\n",
        "\r\n",
        "#add weight decay to convolution and any layer that has bias regulizer (Dense Layers)\r\n",
        "for layer in model.layers:\r\n",
        "    if isinstance(layer, keras.layers.Conv2D) or isinstance(layer, keras.layers.Dense):\r\n",
        "        layer.add_loss(lambda: keras.regularizers.l1_l2(alpha)(layer.kernel))\r\n",
        "    if hasattr(layer, 'bias_regularizer') and layer.use_bias:\r\n",
        "        layer.add_loss(lambda: keras.regularizers.l1_l2(alpha)(layer.bias))\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GscjNrPPgU3T"
      },
      "source": [
        "#Define Callback funtions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwU8x68mgVIs"
      },
      "source": [
        "#Define call back funtions\r\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\r\n",
        "#reduces learning rate\r\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=0.000005)\r\n",
        "#stops the model fitting if the val_loss is not decreasing in 5 epochs\r\n",
        "early_stop = EarlyStopping(monitor='val_accuracy', mode='max', patience = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT7C5_S2gZh7"
      },
      "source": [
        "#Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrBvDhu1gX2t"
      },
      "source": [
        "#Define optimiser for the model and compile the CNN module\r\n",
        "from keras import optimizers\r\n",
        "#model.load_weights('/content/drive/MyDrive/Saved Weights/training_2/cp-0006.ckpt#')\r\n",
        "optimizer=optimizers.Adam(learning_rate=learning_rate)\r\n",
        "#compile the model\r\n",
        "model.compile(optimizer=optimizer,loss=\"binary_crossentropy\", metrics=['accuracy', 'AUC'])\r\n",
        "#train the model\r\n",
        "print(X_train.shape)\r\n",
        "h = model.fit(\r\n",
        "    datagen.flow(X_train,Y_train, batch_size=32),\r\n",
        "    epochs=15,    \r\n",
        "    validation_data= datagen.flow(X_val, Y_val),\r\n",
        "    callbacks=[reduce_lr, early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjF_5RKtgbM0"
      },
      "source": [
        "#Evaluate 1D\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HJ5x1nygbd0"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "y_pred = model.predict(X_test, batch_size=32,verbose=1)\r\n",
        "plt.plot(y_pred)\r\n",
        "plt.show()\r\n",
        "y_pred = (y_pred > 0.5).astype(int)\r\n",
        "print(accuracy_score(Y_test, y_pred))\r\n",
        "print(classification_report(Y_test, y_pred))\r\n",
        "cm = confusion_matrix(Y_test, y_pred)\r\n",
        "print(cm)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}